#!/usr/bin/env perl

# krakenhll-download.pl - based on centrifuge-download
# (c) Florian Breitwieser, 2017-2018
# licensed under GPL-3

use strict;
use warnings;
use File::Basename;
use File::Fetch;
use File::Copy;
use File::Path qw/make_path remove_tree/;
use IO::Uncompress::Gunzip qw/gunzip $GunzipError/;
use autodie;
use Term::ANSIColor;
use Getopt::Long;
use LWP::Simple;
#use LWP::UserAgent;
use List::Util qw/min/;
use FindBin;
use lib "$FindBin::RealBin";
use File::SortedSeek qw/alphabetic/;

sub download_taxonomy(@);
sub download_contaminats(@);
sub download(@);
sub print_header_lines(@);
sub print_header_line_ac(@);
sub download_domain(@);
sub download_viral_neighbors(@);
sub download_ncbi_search(@);
sub download_ncbi_acs(@);
sub get_sorted_maps(@);

my $FTP="ftp://ftp.ncbi.nih.gov";
my @ALL_GENOMES=qw/bacteria viral archaea fungi protozoa invertebrate plant vertebrate_mammalian vertebrate_other/;
my %valid_domains = map { $_ => 1 } @ALL_GENOMES;
my @ALL_DATABASES=qw/refseq genbank taxonomy contaminants/;
my @ALL_ASSEMBLY_LEVELS=qw/Complete_Genome Chromosome Scaffold Contig/;
my @SMALL_GENOMES=qw/mitochondrion plasmid plastid/;

## Option parsing
my $DATABASE="refseq";
my $ASSEMBLY_LEVEL="Complete_Genome";
my $REFSEQ_CATEGORY;
my $TAXID;

my $BASE_DIR;
my $DB_DIR;
my $N_PROC=5;
my $MAP_DIV;
my $CHANGE_HEADER=0;
my $RETMODE="text";
my $RETTYPE="fasta";
my $NCBIDB="nuccore";
my $WRITE_MAPS=1;
my $DO_DUST=0;
my $FILTER_UNPLACED=0;
my $SEARCH_TERM;
my $VERBOSE=0;
my $OVERWRITE_FILES=0;
my $DOMAINS;
my $DL_MOD_RSYNC;
my $n_children = 0;
my $NUC_AC;
my %taxid_name_map;
my @pids;

my $downloaded_viral_refseq=0;
my $FNA_FILES="genomic";
my $vir_nbr_search_term = "viruses[organism] not cellular organisms[orgn] not wgs[prop] not gbdiv syn[prop] and (nuccore genome samespecies[filter] and (complete[title]) not unverified[title])";

my $USAGE="\n".basename($0).
" [<options>] <pattern> <pattern>*

ARGUMENTS
 <pattern> can be one of
     'contaminants'     Contaminant sequences from UniVec and EmVec.
     'taxonomy'         NCBI taxonomy mappings from ftp://ftp.ncbi.nlm.nih.gov/pub/taxonomy/
     'nucleotide'       Download nucleotide sequences using a query specified using --term.
     'viral-neighbors'  Download viral strain sequences from the NCBI Viral Genome Resource.
                        (Search: \"$vir_nbr_search_term\").
     'genbank/DOMAIN'   Download all complete genomes for DOMAIN from GenBank.
     'refseq/DOMAIN'    Download all complete genomes for DOMAIN from RefSeq.
     'refseq/DOMAIN/ASS_LEVEL'
     'refseq/DOMAIN/ASS_LEVEL/COLUMN=value1(/COLUMN=value2)*' 
        Possible values for DOMAIN: @ALL_GENOMES.
        Possible values for ASS_LEVEL: Any, Complete_Genome, Chromosome, Scaffold, Contig.
        Possible values for COLUMN: Any column in the NCBI assembly_summary.txt, e.g. species_taxid or assembly_accession.
        Example: 'refseq/vertebrate_mammalian/Any/species_taxid=9606' <- download all human assemblies

COMMON OPTIONS
 -o <directory>     Folder to which the files are downloaded. Default: '.'
 --db <directory>   Alternative to -o: Download to <directory>/{library,taxonomy}.
 --threads <# of threads>  Number of processes when downloading (uses xargs). Default: '$N_PROC'
 --rsync, -R        Download using rsync.
 --overwrite        Redownload and overwrite files with the same name.
 --verbose          Be verbose.

WHEN USING DATABASE nucleotide or viral-neighbors:
 --search SEARCH_TERM  Download all sequences returned from a NCBI nucleotide search.
                       When used with viral-neighbors, it subsets the viral genomes by the search.
                       E.g. \"txid1570291[Organism]\" for Ebola virus sequences (taxonomy ID 1570291).
 --ac AC1,AC2          Alternative to --search. Download specified ACs of nucleotide database.
 --mapping-file MAP    Map accessions using the specified mapping file(s) (comma-separated).
                       Possible values: nucl_est, nucl_gb, nucl_gss, nucl_wgs.
                       For viral-neighbors, the default is nucl_gb. Unset by giving it an empty string.
                       Downloaded from ftp://ftp.ncbi.nlm.nih.gov/pub/taxonomy/accession2taxid/.
 --retmode RM          Specify return mode. Possible options: text (default), asn.1, xml.
 --rettype RT          Specify return type for download. Note that no mapping files are generated when
                       rettype is specified. Possible options: fasta (default), gb, gbc, native, acc, seqid, ft, 
                       gbwithparts, fasta_cds_na, fasta_cds_aa. Note that only gb and fasta files are split, while the other formats stay in chunks.
                       The resulting files will have the rettype as extension.

WHEN USING DATABASE refseq OR genbank:
 --fna <seq types>  Comma-separated list of sequence types, including genomic, rna, rna_from_genomic, cds_from_genomic. Default: $FNA_FILES.
                    See the assembly project FTP site for available sequences
 -u                 Filter unplaced sequences.
 -l                 Modify sequence header to include taxonomy ID for Kraken (i.e. add '>kraken:taxid|TAXID' to each sequence).
 --dust, -D         Mask low-complexity regions using dustmasker.
";

# arguments: $OPTFIND (current index), $OPTARG (argument for option), $OPTERR (bash-specific)
Getopt::Long::Configure('no_auto_abbrev','pass_through');
GetOptions(
  "output|o=s"  =>\$BASE_DIR,
  "db=s" => \$DB_DIR,
  "threads|P=i" =>\$N_PROC,
  "domain|d=s"  => \$DOMAINS,
  "assembly-level|a=s" => \$ASSEMBLY_LEVEL,
  "category|c=s" => \$REFSEQ_CATEGORY,
  "taxonomy-id|t=s" => \$TAXID,
  "fna=s" => \$FNA_FILES,
  "rsync|R" => \$DL_MOD_RSYNC,
  "filter-unplaced|u" => \$FILTER_UNPLACED,
  "search=s" => \$SEARCH_TERM,
  "term=s" => \$SEARCH_TERM,
  "ac=s" => \$NUC_AC,
  "mapping-file=s" => \$MAP_DIV,
  "dust|D" => \$DO_DUST,
  "change-header|l" => \$CHANGE_HEADER,
  "rettype=s" => \$RETTYPE,
  "retmode=s" => \$RETMODE,
  "ncbidb=s" => \$NCBIDB,
  "force" => \$OVERWRITE_FILES,
  "verbose|v" => \$VERBOSE) or die "Error in command line arguments";

if (defined $BASE_DIR && defined $DB_DIR) {
  print "Define either --db or -o, not both!";
  exit 1;
}

#my $ua = LWP::UserAgent->new( ssl_opts => { verify_hostname => 0 } );

# Use current directory as base directory
$BASE_DIR = "." unless defined $DB_DIR || defined $BASE_DIR;

# If DB directory is defined, use that as base directory
#  -- kept -o and --db options to allow the use of either Kraken and Centrifuge type command line
my $add_dir = defined $DB_DIR;
$BASE_DIR = $DB_DIR if defined $DB_DIR;
sub get_dir {
  my ($dir, $name) = @_;
  my $dir1 = $add_dir? "$dir/$name" : $dir;
  make_path $dir1;
  return $dir1;
}

my %select_taxonomy_ids;
if (defined $TAXID) {
  %select_taxonomy_ids = map { $_ => 1 } split(/,/, $TAXID);
}

if (!defined $ARGV[0]) {
  print STDERR $USAGE;
  exit 1;
}

foreach my $DATABASE (@ARGV) {
  if ( $DATABASE eq "taxonomy" ) { 
    download_taxonomy(get_dir($BASE_DIR,"taxonomy"));
  } elsif ( $DATABASE eq "contaminants" ) { 
    download_contaminats(get_dir($BASE_DIR,"library/contaminants"));
  } elsif ( $DATABASE =~ /^refseq/ || $DATABASE =~ /^genbank/ ) {
    my ($db, $domains, $assembly_levels, @additional_filters) = split(/\//, $DATABASE);
    $domains = $DOMAINS unless defined $domains;
    $assembly_levels = $ASSEMBLY_LEVEL unless defined $assembly_levels;

    foreach my $domain (split(/,/, $domains)) {
      my $lib_dir = $add_dir? "$BASE_DIR/library/$domain" : "$BASE_DIR/$domain";
      foreach my $assembly_level (split(/,/, $assembly_levels)) {
          download_domain($db, $lib_dir, $domain, $assembly_level, @additional_filters);
      }
    }
  } elsif ($DATABASE eq 'viral-neighbors') {
    my $nbr_lib_dir = $add_dir? "$BASE_DIR/library/viral/Neighbors" : "$BASE_DIR/viral/Neighbors";
    my $addon_search_term = $SEARCH_TERM;
    download_viral_neighbors($nbr_lib_dir, $addon_search_term);
  } elsif ($DATABASE eq 'nucleotide' || $DATABASE eq 'assembly' || $DATABASE eq 'genome') {
    my $ncbi_db = $DATABASE;
    $ncbi_db = "nuccore" if $ncbi_db eq "nucleotide";
    my $nbr_dir = $add_dir? "$BASE_DIR/library/$DATABASE/" : "$BASE_DIR/$DATABASE/";

    if (!defined $NUC_AC && !defined $SEARCH_TERM) {
      print STDERR "Please define a search term with --search when using database nucleotide, or provide ACs with --ac. \n".
                   " E.g. --search 'srcdb_refseq[prop] plasmid[title] complete sequence[title] txid2[organism]'\n.";
      exit 1;
    }

    if (defined $NUC_AC) {
      if ($DATABASE eq 'assembly') {
        download_ncbi_search($nbr_dir, $ncbi_db, $NUC_AC."[Assembly Accession]")
      } else {
        download_ncbi_acs($nbr_dir, $ncbi_db, $NUC_AC, get_sorted_maps($MAP_DIV));
      }
    } 
    
    if (defined $SEARCH_TERM) {
      download_ncbi_search($nbr_dir, $ncbi_db, $SEARCH_TERM, get_sorted_maps($MAP_DIV));
    }
  } else {
    print STDERR "Unknown database $DATABASE. \n";
    print STDERR $USAGE;
    exit 1;
  }
}





#########################################################
## Functions

sub download(@) {
  my ($url, $file, $opts) = @_;
  my $gunzipped_filename;

  if (defined $opts && defined $opts->{'gunzip'}) {
    ($gunzipped_filename = $file) =~ s/.gz$//; 
  }

  if (defined $opts && defined $opts->{'verbose'} && !$VERBOSE) {
    print STDERR "$file ";
  }

  if (!$OVERWRITE_FILES && (( defined $gunzipped_filename && -s $gunzipped_filename) || (!defined $gunzipped_filename && -s $file))) {
    print STDERR "Not fetching $url - file $file exists.\n" if $VERBOSE;
      if (defined $opts && defined $opts->{'verbose'} && !$VERBOSE) {
      print colored("check\n", "green");
    }
    return 1;
  }
  if (defined $opts && defined $opts->{'verbose'} && !$VERBOSE) {
    print STDERR ": downloading ...";
  }

  if ($url =~ /^http/) {
    print STDERR "Fetching $url to $file ..." if $VERBOSE;
    if (!-d dirname($file)) {
      make_path(dirname($file));
    }
    $url =~ s/\[/%5B/g;
    $url =~ s/\]/%5D/g;
    #my $response = $ua->get($url, ':content_file' => $file);
    #if (!$response->is_success) {
    #  my $status_line = $response->status_line;
    #  print STDERR "\nFAIL: Error using LWP for downloading $url ($status_line) - tying curl.\n";
      system("curl '$url' -o $file") == 0 or die "Error fetching $url. Is curl installed?\n";
    #} else {
    #  print STDERR "SUCCESS\n" if $VERBOSE;
    #}
  } else {
    if ( $DL_MOD_RSYNC && $url =~ /^ftp/ ) {
     $url =~ s/^ftp/rsync/;
    }
    print STDERR "Fetching $url to $file ..." if $VERBOSE;

    my $ff = File::Fetch->new(uri=>"$url");
    my $where = $ff->fetch(to=> dirname($file));
    if (!defined $where) {
       print STDERR "Error downloading $url.\n";
       print STDERR $ff->error if $VERBOSE;
       return undef;
    }
    move($where, $file);
  }

  if (defined $gunzipped_filename && $gunzipped_filename ne $file) {
    if (defined $opts && defined $opts->{'verbose'} && !$VERBOSE) {
      print STDERR " gunzipping ...";
    }
    print STDERR " GUNZIPPING" if $VERBOSE;
    gunzip $file => $gunzipped_filename or die "gunzip failed: $GunzipError";
    unlink $file;
    $file = $gunzipped_filename;
  }
  if (-s $file || (defined $gunzipped_filename && -s $gunzipped_filename)) {
    print STDERR " SUCCESS\n" if $VERBOSE;
    if (defined $opts && defined $opts->{'verbose'} && !$VERBOSE) {
      print STDERR " done.\n";
    }
  } else {
    print STDERR "failed.\n";
  }

  #my $where = $ff->fetch(to=> dirname($file)) or die "\n$ff->error for $url!";
  return -s $file;
}

sub start_fork() {
  my $pid;
  return if $N_PROC <= 1;
  if ($n_children == $N_PROC) {
    $pid = wait();
    --$n_children;
  }
  if (defined($pid = fork())) {
    if ($pid) {
      ++$n_children;
      #print STDERR "Parent: forked child $pid\n";
      push @pids, $pid;
    } 
  } else {
    print STDERR "ERROR: Failed to fork\n";
  }
  return $pid;
}

sub wait_children() {
  foreach my $pid (@pids) {
    waitpid $pid, 0;
  }
  @pids = ();
  $n_children = 0;
}

sub end_fork() {
  exit() unless $N_PROC <= 1;
}

sub link_to_taxonomy(@) {
  my ($db1, $id_list) = @_;
  my $db2 = 'taxonomy';     # &db
  my $linkname = 'link_to_tax';

  #assemble the esearch URL
  my $base = 'https://eutils.ncbi.nlm.nih.gov/entrez/eutils/';
  my $url = $base . "esearch.fcgi?db=$db1&term=$id_list";

  #post the esearch URL
  print STDERR $url,"\n";
  my $output = get($url);
  print STDERR $output;

  #parse WebEnv and QueryKey
  my $web1 = $1 if ($output =~ /<WebEnv>(\S+)<\/WebEnv>/);
  my $key1 = $1 if ($output =~ /<QueryKey>(\d+)<\/QueryKey>/);

  #assemble the elink URL
  $base = 'https://eutils.ncbi.nlm.nih.gov/entrez/eutils/';
  $url = $base . "elink.fcgi?dbfrom=$db1&db=$db2&query_key=$key1";
  $url .= "&WebEnv=$web1&linkname=$linkname&cmd=neighbor_history";

  #post the elink URL
  print STDERR $url,"\n";
  $output = get($url);
  print STDERR $output;


  #parse WebEnv and QueryKey
  my $web2 = $1 if ($output =~ /<WebEnv>(\S+)<\/WebEnv>/);
  my $key2 = $1 if ($output =~ /<QueryKey>(\d+)<\/QueryKey>/);

  ### include this code for ESearch-ELink-ESummary
  #assemble the esummary URL
  $url = $base . "esummary.fcgi?db=$db2&query_key=$key2&WebEnv=$web2";

  #post the esummary URL
  print STDERR $url,"\n";
  my $docsums = get($url);
  print "$docsums";

  ### include this code for ESearch-ELink-EFetch
  #assemble the efetch URL
  $url = $base . "efetch.fcgi?db=$db2&query_key=$key2&WebEnv=$web2";
  $url .= "&rettype=xml&retmode=xml";
  print STDERR $url,"\n";

  #post the efetch URL
  my $data = get($url);
  print "$data";
}

sub get_taxid(@) {
  my ($ncbi_database, $nbr_ac, @sorted_map_file_handles) = @_;
  my $taxid;
  foreach (@sorted_map_file_handles) {
    my $tell = alphabetic(*$_, $nbr_ac);
    next unless defined $tell;
    $taxid = <$_>;
    $taxid =~ s/.*\t//;
    chomp $taxid;
  }
  
  if (!defined $taxid || !defined $taxid_name_map{$taxid}) {
    link_to_taxonomy($ncbi_database, $nbr_ac);
		exit 1;
    #($taxid) = split /\t/, $res;
  }
  
  if (!defined $taxid) {
    print STDERR "\nNo taxid mapping for sequence $nbr_ac!\n";
  }
  return $taxid;
}

sub download_ncbi_acs(@) {
  my ($nbr_dir, $ncbi_database, $acs, @sorted_map_files) = @_;
  my @acs = split(/,/, $acs);

  my @sorted_map_file_handles;
  foreach (@sorted_map_files) {
    open(my $FH, "<", $_) or die $!;
    push @sorted_map_file_handles, $FH;
  }

  initialize_name_map();

  my $efetch_url="https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=$ncbi_database&rettype=$RETTYPE&id=";
  print STDERR "Downloading ".scalar(@acs)." sequences into $nbr_dir.\n";
  foreach my $ac (@acs) {
    my $taxid = get_taxid($ncbi_database, $ac, @sorted_map_file_handles);
    die "TaxID for $ac??" unless defined $taxid;
    my $name = $taxid_name_map{$taxid};
    die "Name for $ac??" unless defined $name;
    (my $name1 = $name) =~ s/[^a-zA-Z0-9_]/_/g;
    $name1 =~ s/__/_/g;
    system("mkdir -p $nbr_dir/$name1-tax$taxid");
    my $file = "$nbr_dir/$name1-tax$taxid/$ac.$RETTYPE";
    download($efetch_url.$ac, $file);
    print_header_line_ac($file, $ac, $taxid);
  }
  foreach (@sorted_map_file_handles) {
    close $_;
  }
#  $pm->wait_all_children();
}

sub download_ncbi_search(@) {
  my ($nbr_dir, $ncbi_database, $search_term, @sorted_map_files) = @_;
  return unless defined $search_term;
  $search_term =~ s/ /+/g;

  my @sorted_map_file_handles;
  foreach (@sorted_map_files) {
    open(my $FH, "<", $_) or die $!;
    push @sorted_map_file_handles, $FH;
  }

  initialize_name_map();

  my $esearch_url="https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi";
  my $complete_url="$esearch_url?db=$ncbi_database&usehistory=y&retmax=1&retmode=json&term=$search_term";
  my $esearch_file = "$nbr_dir/esearch_res.json";
  download($complete_url, $esearch_file);

  my $n_res=`grep -m1 '"count":' $esearch_file | sed -e 's/.*: "//' -e 's/",.*//'`;
  chomp $n_res;
  print STDERR "Downloading $n_res sequences into $nbr_dir.\n";
  return if (!defined $n_res || $n_res eq 0);

  # Step 2: Download FASTAs (or other formats), 10k at a time
  my $querykey = `grep "querykey" $esearch_file`;
  $querykey =~ /"querykey": "(.*)"/;
  $querykey = $1;

  my $webenv = `grep "webenv" $esearch_file`;
  $webenv =~ /"webenv": "(.*)"/;
  $webenv = $1;

  my $url_params="query_key=$querykey&webenv=$webenv";
  die "Error getting sequences with URL $complete_url . Check $esearch_file." if (!defined $url_params || $url_params eq "");
  print STDERR $url_params."\n";
  my $retstart = 0;
  my $retmax = 10000;
  my @all_fas = ();
  while ($retstart < $n_res) {

    my $curr_retmax = min($n_res, $retstart+$retmax);
    print STDERR "\r  Downloading sequences ".($retstart+1)." to $curr_retmax of $n_res ..." unless $VERBOSE;

    #start_fork() and next;
    my $part_file = "$nbr_dir/${ncbi_database}-results_".($retstart+1)."-to-$curr_retmax.${RETTYPE}";
    download("https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=$ncbi_database&$url_params&rettype=$RETTYPE&retmode=$RETMODE&retstart=$retstart&retmax=$curr_retmax", $part_file);
    my $is_fasta = $RETTYPE eq 'fasta';
    my $is_genbank = $RETTYPE eq 'gb';

    if ($is_fasta || $is_genbank) {
      my $FA_HANDLE;
      my $fa_handle_open = 0;
      open (my $F, "<", $part_file) or die "Couln't open downloaded file";
      while (my $line = <$F>) {
        next if $line eq "\n";
        my $is_header = ($is_fasta && $line =~ /^>/ ) || ($is_genbank && $line =~ /^LOCUS/);
        if ($is_header) {
          my $nbr_ac = $line;
          chomp $nbr_ac;
          if ($is_fasta) {
            $nbr_ac =~ s/^>//;
          } else { #genbank
            $nbr_ac =~ s/^LOCUS\s*//;
          }
          $nbr_ac =~ s/[. ].*//;
     
          my $taxid;
          foreach (@sorted_map_file_handles) {
            my $tell = alphabetic(*$_, $nbr_ac);
            next unless defined $tell;
            $taxid = <$_>;
            $taxid =~ s/.*\t//;
            chomp $taxid;
          }
  
          if (!defined $taxid || !defined $taxid_name_map{$taxid}) {
            my $res = `curl -s "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=$ncbi_database&id=$nbr_ac&rettype=fasta&retmode=xml" | head -n 12  | egrep '<TSeq_taxid>|<TSeq_orgname>'  | sed -e 's#</.*>##' -e 's#.*<.*>##' | paste -sd\$'\\t'`;
            chomp $res;
            ($taxid) = split /\t/, $res;
          }
  
          my $name = $taxid_name_map{$taxid};
          if (!defined $taxid || !defined $name) {
            print STDERR "\nNo mapping for sequence $nbr_ac - not writing it to it's own file!\n";
            close($FA_HANDLE) if $fa_handle_open;
            $fa_handle_open = 0;
            next;
          }
          (my $name1 = $name) =~ s/[^a-zA-Z0-9_]/_/g;
          $name1 =~ s/__/_/g;
          system("mkdir -p $nbr_dir/$name1-tax$taxid");
          if ($fa_handle_open) {
            close($FA_HANDLE);
          } else {
            $fa_handle_open = 1;
          }
          my $file = "$nbr_dir/$name1-tax$taxid/$nbr_ac.$RETTYPE";
          open($FA_HANDLE, ">", $file) or die "Couln't open file";
          print_header_line_ac($file, $nbr_ac, $taxid);
        }
        print $FA_HANDLE $line if $fa_handle_open;
        #end_fork();
      }
      close($FA_HANDLE) if $fa_handle_open;
      close($F);
      unlink($part_file) or warn "Could not delete $part_file: $!\n";
    }
    $retstart += $curr_retmax;
    print STDERR " done \n";
    wait_children();
  
  }
  foreach (@sorted_map_file_handles) {
    close $_;
  }
#  $pm->wait_all_children();
}

sub get_sorted_maps(@) {
  my ($maps_definition) = @_;
  return unless defined $maps_definition;
  return if $maps_definition eq "";
  my @res;
  my $ac_col = 1;
  my $ac_w_version_col = 2;
  my $taxid_col = 3;
  my $dir = get_dir($BASE_DIR,"taxonomy");
  foreach (split(/,/, $maps_definition)) {
    next if $_ eq "";
    my $url = "ftp://ftp.ncbi.nlm.nih.gov/pub/taxonomy/accession2taxid/$_.accession2taxid.gz";
    download($url, "$dir/$_.accession2taxid.gz", { verbose => 1 });

    my $sorted_map_f = "$dir/$_.accession2taxid.sorted";
    if (!-s $sorted_map_f) {
      print STDERR "Sorting mapping file (will take some time) ...\n";
      my $sort_cmd = system("sort --help | grep -q parallel") == 0? "sort --parallel $N_PROC" : "sort";
      system("gunzip -c $dir/$_.accession2taxid.gz | cut -f $ac_col,$taxid_col | $sort_cmd -T $dir > $sorted_map_f") == 0 or die "Error sorting: $!";
    }
    push @res, $sorted_map_f;
  }
  return @res;
}

sub initialize_name_map() {
  return if %taxid_name_map;
  print STDERR "Reading names file ...\n";
  my $dir = get_dir($BASE_DIR,"taxonomy");
  my $names_file = "$dir/names.dmp";
  if (!-f $names_file) {
    download_taxonomy($dir);
  }
  open (my $N, "<", $names_file);
  while (<$N>) {
    next unless /scientific name/;
    my ($taxid, $name) = split /\t\|\t/;
    $taxid_name_map{$taxid} = $name;
  }
  close($N);
}

sub download_viral_neighbors(@) {
  my ($nbr_dir, $addon_search_term) = @_;
  print STDERR "Downloading viral neighbors.\n";
  my $search_term = $vir_nbr_search_term;
  $search_term .= "+AND+$addon_search_term" if defined $addon_search_term;
  my @maps = defined $MAP_DIV? get_sorted_maps($MAP_DIV) : get_sorted_maps("nucl_gb");
  download_ncbi_search($nbr_dir, "nuccore", $search_term, @maps);
}

sub print_header_lines(@) {
  my ($file, $taxid, $name) = @_;
  return if -s "$file.map";
  if (! -f $file) {
    print STDERR "ERROR: $file does not exist - should not happen!?\n";
  }
  print STDERR "Writing mapping file for $file [$taxid, $name]\n" if $VERBOSE;
  $taxid = "$taxid\t$name" if defined $name;
  #`grep '^>' "$file" | sed -e 's/.//' -e 's/\\( .*\\|\$\\)/\t$taxid/' > "$file.map"`;
  open (my $F, ">", "$file.map");
  open (my $G, "<", $file);
  while (<$G>) {
   next unless /^>([^ ]*)/;
    my $ac = $1;
    print $F "$ac\t$taxid\n";
  }
  close($G);
  close($F);
}


sub print_header_line_ac(@) {
  my ($file, $ac, $taxid, $name) = @_;
  return if -s "$file.map";
  print STDERR "Making map file for $file\n" if ($VERBOSE);
  open (my $F, ">", "$file.map");
  if (defined $name) {
    print $F "$ac\t$taxid\t$name\n";
  } else {
    print $F "$ac\t$taxid\n";
  }
  close($F);
}


sub download_contaminats(@) {
  my ($CONTAMINANT_DIR) = @_;
  print STDERR "Downloading contaminant databases ... \n";
  my $CONTAMINANT_TAXID=32630;
  make_path $CONTAMINANT_DIR;

  # download UniVec and EmVec database
  download("ftp://ftp.ncbi.nlm.nih.gov/pub/UniVec/UniVec","$CONTAMINANT_DIR/UniVec.fna");
  download("ftp://ftp.ebi.ac.uk/pub/databases/emvec/emvec.dat.gz","$CONTAMINANT_DIR/emvec.dat.gz", { gunzip => 1 });

  open(my $E1, "<", "$CONTAMINANT_DIR/emvec.dat");
  open(my $E2, ">", "$CONTAMINANT_DIR/EmVec.fna");

  my ($ac,$de);
  my $in_seq = 0;
  while(<$E1>) {
    if (/^AC\s+(.*)/) {
      $ac = $1;
      $ac =~ s/;$//;
    } elsif (/^DE\s+(.*)/) {
      $de = $1;
   } elsif (/^SQ/) {
      $in_seq = 1;
      print $E2 ">$ac $de\n";
    } elsif ($in_seq) {
      if (/^\s+[agct]/) {
        s/\s+[0-9]+$//;
       s/ //g;
       print $E2 $_;
      } else {
        $in_seq = 0;
      }
    }
  }
  close($E2);
  close($E1);
  unlink("$CONTAMINANT_DIR/emvec.dat");
 
  if ( $WRITE_MAPS ) {
    print_header_lines("$CONTAMINANT_DIR/UniVec.fna", $CONTAMINANT_TAXID, "UniVec");
    print_header_lines("$CONTAMINANT_DIR/EmVec.fna", $CONTAMINANT_TAXID, "EmVec");
  }
}

sub download_taxonomy(@) {
  my ($dir) = @_;
  print STDERR "Downloading NCBI taxonomy ... \n";
  make_path $dir;

  download("$FTP/pub/taxonomy/taxdump.tar.gz", "$dir/taxdump.tar.gz");
  system("tar -C $dir -zxvf $dir/taxdump.tar.gz nodes.dmp names.dmp 1>&2");
  system("date > $dir/timestamp");
}

sub download_domain(@) {
  my ($DATABASE, $domain_dir, $domain, $_assembly_level, @additional_filters) = @_;
  $_assembly_level =~ s/ /_/g;
  print STDERR "Downloading assembly summary file for $domain genomes, and filtering to assembly level $_assembly_level";
  print STDERR (@additional_filters? " and additional filters @additional_filters.\n" : ".\n");
  die "Don't know domain $domain. Choose one of the following: ".(join(", ", @ALL_GENOMES))."\n" unless defined $valid_domains{$domain};

  die unless defined $domain_dir && defined $domain;
  if (-d $domain_dir) {
    print STDERR "WARNING: $domain_dir already exists - potentially overwriting files.\n";
  } else {
    make_path $domain_dir;
  }
  my $ass_file = "$domain_dir/assembly_summary.txt";
  my $ass_file_filtered = "$domain_dir/assembly_summary_filtered.txt";
  my $n_genomes = 0;
  download("ftp://ftp.ncbi.nlm.nih.gov/genomes/$DATABASE/$domain/assembly_summary.txt", $ass_file) or die "Could not download assembly summary file!";

  my $is_viral_refseq =1 if $domain eq "viral" && $DATABASE eq "refseq";

  my %cols = (
    assembly_accession=>0,
    bioproject=>1,
    biosample=>2,
    wgs_master=>3,
    refseq_category=>4,
    taxid=>5,
    species_taxid=>6,
    organism_name=>7,
    infraspecific_name=>8,
    isolate=>9,
    version_status=>10,
    assembly_level=>11,
    release_type=>12,
    genome_rep=>13,
    seq_rel_date=>14,
    asm_name=>15,
    submitter=>16,
    gbrs_paired_asm=>17,
    paired_asm_comp=>18,
    ftp_path=>19,
    excluded_from_refseq=>20,
    relation_to_type_material=>21
  );

  my %seen_acs;
  my @genomes_to_dl;
  open(my $A1, "<", $ass_file);
  open(my $A2, ">", $ass_file_filtered);
  while (<$A1>) {
    next if /^#/;
    my @fields = split /\t/;

    $fields[$cols{"assembly_level"}] =~ s/ /_/g;
    next unless $fields[$cols{"version_status"}] eq "latest";
    next if ($_assembly_level ne "Any" && $fields[$cols{"assembly_level"}] ne $_assembly_level);
    next if (defined $REFSEQ_CATEGORY && $fields[$cols{"refseq_category"}] ne $REFSEQ_CATEGORY);

    ## Kick out duplicates
    next if (defined $seen_acs{$fields[$cols{"assembly_accession"}]});
    $seen_acs{$fields[$cols{"assembly_accession"}]} = 1;
    
    my $keep_it = 1;
    foreach (@additional_filters) {
        my ($k, $v) = split(/=/);
        die "$k is not an available column filter. Available: ".(join(", ", sort keys %cols)) if (!defined $cols{$k});
        $keep_it = 0 if $fields[$cols{$k}] ne $v; 
    }
    next unless $keep_it;

    print $A2 $_;
    ++ $n_genomes;
    push @genomes_to_dl, [
            $fields[$cols{"ftp_path"}], $fields[$cols{"taxid"}], $fields[$cols{"organism_name"}], 
            $fields[$cols{"infraspecific_name"}], $fields[$cols{"assembly_accession"}], $fields[$cols{"assembly_level"}]];
  }
  close $A2;
  close $A1;

  my $downloaded_files = 0;
  my $existing_files = 0;
  my $download_failed = 0;

  my @fasta_files;

  my $i = 0;
  foreach my $g (@genomes_to_dl) {
    my ($ftp_path, $taxid, $organism_name, $infraspecific_name, $assembly_accession, $assembly_level) = @$g;
    ++$i;
    #print STDERR "\r                                                                               " unless $VERBOSE;
    print STDERR "\r Downloading $domain genomes:  $i/$n_genomes ... " unless $VERBOSE;

    if (defined $infraspecific_name) {
        (my $i1 = $infraspecific_name) =~ s/strain=//;
        $organism_name .= " $infraspecific_name" unless $organism_name =~ /\Q$i1\E/ || $i1 eq "";
    }


    my $bname = basename($ftp_path);
    ( my $organism_name1 = $organism_name ) =~ s/[^a-zA-Z0-9_]/_/g;
    $organism_name1 = substr($organism_name1, 0, 100);
    $organism_name1 =~ s/__/_/g;
    $organism_name1 =~ s/_$//;
    my $bname1 = "${organism_name1}-tax${taxid}-${bname}";
    
    $assembly_level =~ s/ /_/g;
    my $download_dir = "$domain_dir/$assembly_level";
    my $nbr_download_dir = "$domain_dir/Neighbors";
    my @files;
    foreach my $ext (split(/,/, $FNA_FILES)) {
      my $full_ftp_path = "$ftp_path/${bname}_${ext}.fna.gz";
      my $bfname = $bname1."_".$ext;
      my $fname = $bfname.".fna";
      my $fullfname1 = $DO_DUST? "$download_dir/${bfname}_dustmasked.fna" : "$download_dir/$fname";

      if (!$OVERWRITE_FILES && -s $fullfname1) {
        print STDERR "$download_dir/$fname exists - not downloading.. \n" if $VERBOSE;
        ++$existing_files;
      } else {
        my $pid = start_fork();
        unless ($pid) {
          download($full_ftp_path, "$download_dir/$fname.gz", { gunzip => 1 });
          ## Output sequenceID to taxonomy ID map
          print_header_lines("$download_dir/$fname", $taxid, "$assembly_accession $organism_name");
          end_fork();
        }
      }
      push @fasta_files, "$download_dir/$fname";
    }
  }

  ## if ($FILTER_UNPLACED) { die("Not implemented"); }
  wait_children();
  foreach my $fasta_file (@fasta_files) {
        if (-s $fasta_file) {
          ++$downloaded_files;
        } else {
          ++$download_failed;
        }
  }


  if ($DO_DUST) {
  foreach my $fasta_file (@fasta_files) {
    ( my $fullfname1 = $fasta_file ) =~ s/.fna$/_dustmasked.fna/;
    if (!$OVERWRITE_FILES && -s $fullfname1) {
      print STDERR "$fullfname1 exists - not dusting.. \n" if $VERBOSE;
    } else {
      ## TODO: Consider hard-masking only low-complexity stretches with 10 or more bps
      my $pid = start_fork();
      unless ($pid) {
        system("dustmasker -infmt fasta -in '$fasta_file' -level 20 -outfmt fasta | sed '/^>/! s/[^AGCT]/N/g' > '$fullfname1'");
        unlink("$fasta_file");
        end_fork();
      }
    }
  }
  wait_children();
  }

  my $msg  = "Found $downloaded_files files.";
  $msg .= " Skipped download of $existing_files files that already existed." if $existing_files > 0;
  $msg .= " $download_failed downloads failed - maybe try re-running krakenhll-download." if ($download_failed > 0);
  print STDERR "  $msg\n";
}
# vim: tabstop=8 expandtab tabstop=2 shiftwidth=2 :
